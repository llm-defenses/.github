## LLM Defenses ðŸ‘‹

The military rule-of-thumb is that defense is easier than offense. Is this so? How would this apply in the case of
attacking and defending LLMs? This GitHub organization is meant to serve as a community repository of approaches
to defending LLMs from adversarial attacks.

Adversarial attacks on LLMs is being actively worked on by major AI companies. A very good example of this work
is the recent paper by researchers at CMU and [Google Deep Mind](https://deepmind.google/)
["Universal and Transferable Adversarial Attacks on Aligned Language Models"](https://arxiv.org/abs/2307.15043).
Their work can be found under the GitHub organization "llm-attacks" [here](https://github.com/llm-attacks].

<!--

**Here are some ideas to get you started:**

ðŸ™‹â€â™€ï¸ A short introduction - what is your organization all about?
ðŸŒˆ Contribution guidelines - how can the community get involved?
ðŸ‘©â€ðŸ’» Useful resources - where can the community find your docs? Is there anything else the community should know?
ðŸ¿ Fun facts - what does your team eat for breakfast?
ðŸ§™ Remember, you can do mighty things with the power of [Markdown](https://docs.github.com/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
-->
